{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0931c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92c37b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Roman Urdu stopwords\n",
    "STOP_WORDS = set([\n",
    "    \"ka\", \"ki\", \"kaun\", \"kon\", \"kya\", \"aur\", \"se\", \"mein\", \"tu\", \"tum\", \"main\", \"wo\", \"woh\",\n",
    "    \"to\", \"mn\", \"ja\", \"rha\", \"ye\", \"yeh\", \"is\", \"ko\", \"tha\", \"thi\", \"ke\", \"ho\", \"raha\", \"rahe\",\n",
    "    \"bhi\", \"par\", \"ab\", \"hain\", \"hun\", \"tak\", \"jab\", \"sirf\", \"liye\", \"chal\", \"gaya\", \"gayi\", \"gai\", \n",
    "    \"wahan\", \"ahan\", \"kyun\", \"kis\", \"hona\", \"hoti\", \"hota\", \"kar\", \"karo\", \"karta\", \"karte\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6783325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = [w for w in text.split() if w not in STOP_WORDS]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"C:\\Users\\hp\\OneDrive\\Desktop\\ABS\\University\\Sentiment_Analysis\\Dataset 11000 Reviewss.csv\"  \n",
    "\n",
    "# Reading the CSV dataset\n",
    "df = pd.read_csv(dataset_path, encoding=\"latin1\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f786e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'label': 'sentiment',   # Change if your label column has a different name\n",
    "    'review': 'review'      # Change if your review column has a different name\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "782c01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess reviews\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e95ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.18%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.81      0.81      0.81      1602\n",
      "         pos       0.82      0.82      0.82      1698\n",
      "\n",
      "    accuracy                           0.81      3300\n",
      "   macro avg       0.81      0.81      0.81      3300\n",
      "weighted avg       0.81      0.81      0.81      3300\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1290  312]\n",
      " [ 309 1389]]\n",
      "Predicted Sentiment: neg\n",
      "Predicted Sentiment: pos\n",
      "Predicted Sentiment: pos\n",
      "Predicted Sentiment: pos\n",
      "Predicted Sentiment: neg\n",
      "Predicted Sentiment: pos\n"
     ]
    }
   ],
   "source": [
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "y = df['sentiment']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Prediction function\n",
    "def predict_sentiment(text):\n",
    "    cleaned = preprocess_text(text)\n",
    "    vec = vectorizer.transform([cleaned])\n",
    "    return model.predict(vec)[0]\n",
    "\n",
    "# Test with your own review\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter Roman Urdu review (or 'exit' to stop): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    print(\"Predicted Sentiment:\", predict_sentiment(user_input))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
